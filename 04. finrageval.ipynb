{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    " \n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 1: Load data from source JSON into pandas DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convfinqadfloader(filepath: str, max_rows: int=1000) -> pd.DataFrame:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if max_rows is not None:\n",
    "        data = data[:max_rows]\n",
    "\n",
    "    # Initialize list to store flattened data\n",
    "    flattened_data = []\n",
    "    \n",
    "    # Process each record in the JSON data\n",
    "    for item in data:\n",
    "        # Create base item with common fields\n",
    "        base_item = {\n",
    "            'id': item.get('id'),\n",
    "            'pre_text': ' '.join(item.get('pre_text', [])),\n",
    "            'post_text': ' '.join(item.get('post_text', [])),\n",
    "            'filename': item.get('filename'),\n",
    "            'table': str(item.get('table')),\n",
    "        }\n",
    "\n",
    "    # handle annotation which can be either a dictionary or a list\n",
    "        annotation = item.get('annotation', {})\n",
    "        if isinstance(annotation, dict):\n",
    "            # Extract dialogue information\n",
    "            dialogue_break = annotation.get('dialogue_break', [])\n",
    "            turn_program = annotation.get('turn_program', [])\n",
    "            qa_split = annotation.get('qa_split', [])\n",
    "            exe_ans_list = annotation.get('exe_ans_list', [])\n",
    "        \n",
    "        # Create a row for each turn in the dialogue\n",
    "            for idx in range(len(dialogue_break)):\n",
    "                turn_data = {\n",
    "                    'dialogue_text': dialogue_break[idx] if idx < len(dialogue_break) else None,\n",
    "                    'turn_program': turn_program[idx] if idx < len(turn_program) else None,\n",
    "                    'qa_split': qa_split[idx] if idx < len(qa_split) else None,\n",
    "                    'execution_answer': exe_ans_list[idx] if idx < len(exe_ans_list) else None,\n",
    "                    'turn_index': idx\n",
    "                }\n",
    "                \n",
    "                # Combine base item with turn data\n",
    "                combined_data = {**base_item, **turn_data}\n",
    "                flattened_data.append(combined_data)\n",
    "        \n",
    "        # Handle potential QA pairs stored directly\n",
    "        if 'qa' in item:\n",
    "            qa_data = {\n",
    "                'question': item['qa'].get('question'),\n",
    "                'answer': item['qa'].get('answer'),\n",
    "                'explanation': item['qa'].get('explanation'),\n",
    "                'program': item['qa'].get('program'),\n",
    "                'execution_answer': item['qa'].get('exe_ans'),\n",
    "                'turn_index': 0  # Single QA pair\n",
    "            }\n",
    "            combined_data = {**base_item, **qa_data}\n",
    "            flattened_data.append(combined_data)\n",
    "    \n",
    "    # Create DataFrame from flattened data\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "\n",
    "    # Convert appropriate columns to proper numeric types\n",
    "    numeric_columns = ['turn_index', 'qa_split', 'execution_answer']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 2: Initialise embedding and generator models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(embedding_model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "                     generator_name: str = \"google/flan-t5-base\"):\n",
    "    \"\"\"Initialize all required models.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    embedding_tokenizer = None\n",
    "    generator_tokenizer = None\n",
    "\n",
    "    # Initialize models\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    embedding_model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(generator_name)\n",
    "    generator_model = AutoModelForSeq2SeqLM.from_pretrained(generator_name).to(device)\n",
    "    \n",
    "    embedding_dim = embedding_model.config.hidden_size\n",
    "    \n",
    "    print(f\"Models initialized successfully. Using device: {device}\")\n",
    "    return (embedding_tokenizer, embedding_model, generator_tokenizer, \n",
    "            generator_model, device, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 3: Get Embedding Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, tokenizer, model, device, embedding_dim):\n",
    "    \"\"\"Get embeddings for a text using HuggingFace model.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", \n",
    "                         max_length=512, truncation=True,\n",
    "                         padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "            \n",
    "        return embedding.cpu().numpy().astype(np.float32)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return np.zeros(embedding_dim, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 4: Process and Index Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text_content(row: pd.Series) -> str:\n",
    "    \"\"\"Merge all text content from a DataFrame row into a single coherent text.\"\"\"\n",
    "    merged_parts = []\n",
    "    \n",
    "    if pd.notna(row.get('pre_text')):\n",
    "        merged_parts.append(f\"Background Information: {row['pre_text']}\")\n",
    "    \n",
    "    if pd.notna(row.get('table')):\n",
    "        try:\n",
    "            table_data = eval(row['table'])\n",
    "            table_text = []\n",
    "            for row_idx, table_row in enumerate(table_data):\n",
    "                row_text = \" | \".join(str(cell) for cell in table_row)\n",
    "                table_text.append(f\"Row {row_idx + 1}: {row_text}\")\n",
    "            merged_parts.append(\"Tabular Data: \" + \" \".join(table_text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table: {e}\")\n",
    "    \n",
    "    if pd.notna(row.get('dialogue_text')):\n",
    "        merged_parts.append(f\"Dialogue: {row['dialogue_text']}\")\n",
    "    \n",
    "    if pd.notna(row.get('question')):\n",
    "        merged_parts.append(f\"Question Context: {row['question']}\")\n",
    "    if pd.notna(row.get('answer')):\n",
    "        merged_parts.append(f\"Reference Answer: {row['answer']}\")\n",
    "    \n",
    "    if pd.notna(row.get('post_text')):\n",
    "        merged_parts.append(f\"Additional Context: {row['post_text']}\")\n",
    "    \n",
    "    return \" \".join(merged_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text: str, chunk_size: int = 512, chunk_overlap: int = 128) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks while preserving semantic coherence.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "        \n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            \n",
    "            overlap_size = 0\n",
    "            overlap_chunk = []\n",
    "            for s in reversed(current_chunk):\n",
    "                if overlap_size + len(s) <= chunk_overlap:\n",
    "                    overlap_chunk.insert(0, s)\n",
    "                    overlap_size += len(s)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            current_chunk = overlap_chunk\n",
    "            current_size = overlap_size\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_index_data(df: pd.DataFrame, embedding_tokenizer, embedding_model, \n",
    "                          device, embedding_dim):\n",
    "    \"\"\"Process and index the DataFrame content using FAISS.\"\"\"\n",
    "    print(\"Processing documents...\")\n",
    "    all_chunks = []\n",
    "    chunk_sources = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        merged_text = merge_text_content(row)\n",
    "        text_chunks = create_chunks(merged_text)\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            metadata = {\n",
    "                'id': row.get('id'),\n",
    "                'filename': row.get('filename'),\n",
    "                'turn_index': row.get('turn_index'),\n",
    "                'contains_question': 'Question:' in chunk,\n",
    "                'contains_answer': 'Answer:' in chunk\n",
    "            }\n",
    "            all_chunks.append((chunk, metadata))\n",
    "    \n",
    "    chunks = [chunk[0] for chunk in all_chunks]\n",
    "    chunk_sources = [chunk[1] for chunk in all_chunks]\n",
    "    \n",
    "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "    \n",
    "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        batch_embeddings = [get_embedding(text, embedding_tokenizer, embedding_model, \n",
    "                                        device, embedding_dim) for text in batch]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    embeddings_array = np.array(embeddings, dtype=np.float32)\n",
    "    faiss_index.add(embeddings_array)\n",
    "    \n",
    "    return chunks, chunk_sources, faiss_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 5: Retriever Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, embedding_tokenizer, embedding_model, device, embedding_dim,\n",
    "            chunks, chunk_sources, faiss_index, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieve the k most relevant chunks for a query using FAISS.\"\"\"\n",
    "    query_embedding = get_embedding(query, embedding_tokenizer, embedding_model, \n",
    "                                  device, embedding_dim)\n",
    "    \n",
    "    distances, indices = faiss_index.search(\n",
    "        query_embedding.reshape(1, -1),\n",
    "        k\n",
    "    )\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        retrieved_chunks.append({\n",
    "            'text': chunks[idx],\n",
    "            'similarity': float(1 / (1 + distances[0][i])),\n",
    "            'metadata': chunk_sources[idx]\n",
    "        })\n",
    "    \n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 6: Generate Answer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, retrieved_chunks: List[Dict], \n",
    "                   generator_tokenizer, generator_model, device) -> Dict:\n",
    "    \"\"\"Generate an answer using the retrieved context.\"\"\"\n",
    "    sorted_chunks = sorted(retrieved_chunks, key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    context_parts = []\n",
    "    for chunk in sorted_chunks:\n",
    "        chunk_text = chunk['text']\n",
    "        for marker in ['Question:', 'Answer:', 'Reference Answer:']:\n",
    "            if marker in chunk_text:\n",
    "                chunk_text = chunk_text.split(marker)[0]\n",
    "        context_parts.append(chunk_text.strip())\n",
    "    \n",
    "    context = ' '.join(context_parts)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Based on the following context, please provide a detailed and accurate \"\n",
    "        f\"answer to the question. Consider all relevant information from the context.\\n\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    \n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", \n",
    "                               max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    outputs = generator_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,\n",
    "        min_length=30,\n",
    "        num_beams=4,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    generated_answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    source_documents = {chunk['metadata'].get('filename') for chunk in retrieved_chunks}\n",
    "    \n",
    "    return {\n",
    "        'answer': generated_answer,\n",
    "        'source_documents': list(source_documents),\n",
    "        'context_used': context,\n",
    "        'top_chunk_similarity': retrieved_chunks[0]['similarity'] if retrieved_chunks else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 7: Evaluation Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(test_df: pd.DataFrame, models_and_data: tuple) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the RAG pipeline's performance on a test dataset.\"\"\"\n",
    "    (embedding_tokenizer, embedding_model, generator_tokenizer, generator_model,\n",
    "     device, embedding_dim, chunks, chunk_sources, faiss_index) = models_and_data\n",
    "    \n",
    "    print(\"Evaluating pipeline performance...\")\n",
    "    \n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    metrics = {\n",
    "        'retrieval_precision': [],\n",
    "        'rouge1_f1': [],\n",
    "        'rouge2_f1': [],\n",
    "        'rougeL_f1': [],\n",
    "        'answer_similarity': []\n",
    "    }\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        if pd.isna(row.get('question')) or pd.isna(row.get('answer')):\n",
    "            continue\n",
    "            \n",
    "        retrieved = retrieve(row['question'], embedding_tokenizer, embedding_model,\n",
    "                           device, embedding_dim, chunks, chunk_sources, faiss_index)\n",
    "        \n",
    "        result = generate_answer(row['question'], retrieved, generator_tokenizer,\n",
    "                               generator_model, device)\n",
    "        \n",
    "        relevant_docs = set([row.get('filename')])\n",
    "        retrieved_docs = set(result['source_documents'])\n",
    "        if relevant_docs and retrieved_docs:\n",
    "            precision = len(relevant_docs.intersection(retrieved_docs)) / len(retrieved_docs)\n",
    "            metrics['retrieval_precision'].append(precision)\n",
    "        \n",
    "        rouge_scores = rouge_scorer_instance.score(result['answer'], row['answer'])\n",
    "        metrics['rouge1_f1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "        metrics['rouge2_f1'].append(rouge_scores['rouge2'].fmeasure)\n",
    "        metrics['rougeL_f1'].append(rouge_scores['rougeL'].fmeasure)\n",
    "        \n",
    "        ref_embedding = get_embedding(row['answer'], embedding_tokenizer, embedding_model,\n",
    "                                    device, embedding_dim)\n",
    "        gen_embedding = get_embedding(result['answer'], embedding_tokenizer, embedding_model,\n",
    "                                    device, embedding_dim)\n",
    "        \n",
    "        similarity = cosine_similarity(\n",
    "            ref_embedding.reshape(1, -1),\n",
    "            gen_embedding.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        metrics['answer_similarity'].append(similarity)\n",
    "    \n",
    "    evaluation_results = {\n",
    "        'avg_retrieval_precision': np.mean(metrics['retrieval_precision']),\n",
    "        'avg_rouge1_f1': np.mean(metrics['rouge1_f1']),\n",
    "        'avg_rouge2_f1': np.mean(metrics['rouge2_f1']),\n",
    "        'avg_rougeL_f1': np.mean(metrics['rougeL_f1']),\n",
    "        'avg_answer_similarity': np.mean(metrics['answer_similarity']),\n",
    "        'num_evaluated_samples': len(metrics['rouge1_f1'])\n",
    "    }\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(data_path: str, max_rows: int = 100):\n",
    "    \"\"\"Run the complete pipeline from data loading to evaluation.\"\"\"\n",
    "    # Initialize models\n",
    "    print(\"Initializing models...\")\n",
    "    model_components = initialize_models()\n",
    "    (embedding_tokenizer, embedding_model, generator_tokenizer, \n",
    "     generator_model, device, embedding_dim) = model_components\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    df = convfinqadfloader(data_path, max_rows)\n",
    "    \n",
    "    # Split into train/test\n",
    "    print(\"\\nSplitting data into train/test sets...\")\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "    \n",
    "    # Process and index training data\n",
    "    print(\"\\nProcessing and indexing training data...\")\n",
    "    chunks, chunk_sources, faiss_index = process_and_index_data(\n",
    "        train_df, embedding_tokenizer, embedding_model, device, embedding_dim\n",
    "    )\n",
    "    \n",
    "    # Combine all components needed for evaluation\n",
    "    models_and_data = (embedding_tokenizer, embedding_model, generator_tokenizer,\n",
    "                      generator_model, device, embedding_dim, chunks, chunk_sources,\n",
    "                      faiss_index)\n",
    "    \n",
    "    # Evaluate pipeline\n",
    "    print(\"\\nEvaluating pipeline...\")\n",
    "    evaluation_results = evaluate_pipeline(test_df, models_and_data)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Number of evaluated samples: {evaluation_results['num_evaluated_samples']}\")\n",
    "    print(f\"Average Retrieval Precision: {evaluation_results['avg_retrieval_precision']:.4f}\")\n",
    "    print(f\"Average ROUGE-1 F1: {evaluation_results['avg_rouge1_f1']:.4f}\")\n",
    "    print(f\"Average ROUGE-2 F1: {evaluation_results['avg_rouge2_f1']:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1: {evaluation_results['avg_rougeL_f1']:.4f}\")\n",
    "    print(f\"Average Answer Similarity: {evaluation_results['avg_answer_similarity']:.4f}\")\n",
    "    \n",
    "    return evaluation_results, models_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Models initialized successfully. Using device: cpu\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Splitting data into train/test sets...\n",
      "\n",
      "Processing and indexing training data...\n",
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [00:00<00:00, 9239.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for 3568 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [04:32<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating pipeline...\n",
      "Evaluating pipeline performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87 [00:00<?, ?it/s]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 6/87 [00:11<02:30,  1.86s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 11%|█▏        | 10/87 [00:22<02:59,  2.33s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 15%|█▍        | 13/87 [00:25<02:15,  1.84s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 20%|█▉        | 17/87 [00:27<01:33,  1.34s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 21/87 [00:40<02:11,  1.99s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 24/87 [00:44<01:54,  1.82s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 31%|███       | 27/87 [00:55<02:25,  2.42s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 33/87 [01:06<01:54,  2.12s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 41%|████▏     | 36/87 [01:17<02:09,  2.54s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 51%|█████     | 44/87 [01:29<01:27,  2.03s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 56%|█████▋    | 49/87 [01:33<01:01,  1.61s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 60/87 [01:36<00:26,  1.01it/s]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 74%|███████▎  | 64/87 [01:49<00:33,  1.44s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 71/87 [02:02<00:25,  1.57s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 76/87 [02:06<00:15,  1.38s/it]/Users/adas/miniconda3/envs/llmengg/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 87/87 [02:18<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Number of evaluated samples: 16\n",
      "Average Retrieval Precision: 0.0000\n",
      "Average ROUGE-1 F1: 0.0022\n",
      "Average ROUGE-2 F1: 0.0000\n",
      "Average ROUGE-L F1: 0.0022\n",
      "Average Answer Similarity: 0.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, models_and_data = run_pipeline(\"./data/convfinqatrain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmengg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
