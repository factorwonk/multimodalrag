{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    " \n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "from convfinqaloader import convfinqadfloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 1: Load data from source JSON into pandas DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convfinqadfloader(filepath: str, max_rows: int=1000) -> pd.DataFrame:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if max_rows is not None:\n",
    "        data = data[:max_rows]\n",
    "\n",
    "    # Initialize list to store flattened data\n",
    "    flattened_data = []\n",
    "    \n",
    "    # Process each record in the JSON data\n",
    "    for item in data:\n",
    "        # Create base item with common fields\n",
    "        base_item = {\n",
    "            'id': item.get('id'),\n",
    "            'pre_text': ' '.join(item.get('pre_text', [])),\n",
    "            'post_text': ' '.join(item.get('post_text', [])),\n",
    "            'filename': item.get('filename'),\n",
    "            'table': str(item.get('table')),\n",
    "        }\n",
    "\n",
    "    # handle annotation which can be either a dictionary or a list\n",
    "        annotation = item.get('annotation', {})\n",
    "        if isinstance(annotation, dict):\n",
    "            # Extract dialogue information\n",
    "            dialogue_break = annotation.get('dialogue_break', [])\n",
    "            turn_program = annotation.get('turn_program', [])\n",
    "            qa_split = annotation.get('qa_split', [])\n",
    "            exe_ans_list = annotation.get('exe_ans_list', [])\n",
    "        \n",
    "        # Create a row for each turn in the dialogue\n",
    "            for idx in range(len(dialogue_break)):\n",
    "                turn_data = {\n",
    "                    'dialogue_text': dialogue_break[idx] if idx < len(dialogue_break) else None,\n",
    "                    'turn_program': turn_program[idx] if idx < len(turn_program) else None,\n",
    "                    'qa_split': qa_split[idx] if idx < len(qa_split) else None,\n",
    "                    'execution_answer': exe_ans_list[idx] if idx < len(exe_ans_list) else None,\n",
    "                    'turn_index': idx\n",
    "                }\n",
    "                \n",
    "                # Combine base item with turn data\n",
    "                combined_data = {**base_item, **turn_data}\n",
    "                flattened_data.append(combined_data)\n",
    "        \n",
    "        # Handle potential QA pairs stored directly\n",
    "        if 'qa' in item:\n",
    "            qa_data = {\n",
    "                'question': item['qa'].get('question'),\n",
    "                'answer': item['qa'].get('answer'),\n",
    "                'explanation': item['qa'].get('explanation'),\n",
    "                'program': item['qa'].get('program'),\n",
    "                'execution_answer': item['qa'].get('exe_ans'),\n",
    "                'turn_index': 0  # Single QA pair\n",
    "            }\n",
    "            combined_data = {**base_item, **qa_data}\n",
    "            flattened_data.append(combined_data)\n",
    "    \n",
    "    # Create DataFrame from flattened data\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "\n",
    "    # Convert appropriate columns to proper numeric types\n",
    "    numeric_columns = ['turn_index', 'qa_split', 'execution_answer']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 2: Initialise embedding and generator models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(embedding_model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "                     generator_name: str = \"google/flan-t5-base\"):\n",
    "    \"\"\"Initialize all required models.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize models\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    embedding_model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(generator_name)\n",
    "    generator_model = AutoModelForSeq2SeqGeneration.from_pretrained(generator_name).to(device)\n",
    "    \n",
    "    embedding_dim = embedding_model.config.hidden_size\n",
    "    \n",
    "    print(f\"Models initialized successfully. Using device: {device}\")\n",
    "    return (embedding_tokenizer, embedding_model, generator_tokenizer, \n",
    "            generator_model, device, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 3: Get Embedding Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, tokenizer, model, device, embedding_dim):\n",
    "    \"\"\"Get embeddings for a text using HuggingFace model.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", \n",
    "                         max_length=512, truncation=True,\n",
    "                         padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "            \n",
    "        return embedding.cpu().numpy().astype(np.float32)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return np.zeros(embedding_dim, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 4: Process and Index Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_index_data(df: pd.DataFrame, embedding_tokenizer, embedding_model, \n",
    "                          device, embedding_dim):\n",
    "    \"\"\"Process and index the DataFrame content using FAISS.\"\"\"\n",
    "    print(\"Processing documents...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        merged_text = merge_text_content(row)\n",
    "        text_chunks = create_chunks(merged_text)\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            metadata = {\n",
    "                'id': row.get('id'),\n",
    "                'filename': row.get('filename'),\n",
    "                'turn_index': row.get('turn_index'),\n",
    "                'contains_question': 'Question:' in chunk,\n",
    "                'contains_answer': 'Answer:' in chunk\n",
    "            }\n",
    "            all_chunks.append((chunk, metadata))\n",
    "    \n",
    "    chunks = [chunk[0] for chunk in all_chunks]\n",
    "    chunk_sources = [chunk[1] for chunk in all_chunks]\n",
    "    \n",
    "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "    \n",
    "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        batch_embeddings = [get_embedding(text, embedding_tokenizer, embedding_model, \n",
    "                                        device, embedding_dim) for text in batch]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    embeddings_array = np.array(embeddings, dtype=np.float32)\n",
    "    faiss_index.add(embeddings_array)\n",
    "    \n",
    "    return chunks, chunk_sources, faiss_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 5: Retriever Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, embedding_tokenizer, embedding_model, device, embedding_dim,\n",
    "            chunks, chunk_sources, faiss_index, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieve the k most relevant chunks for a query using FAISS.\"\"\"\n",
    "    query_embedding = get_embedding(query, embedding_tokenizer, embedding_model, \n",
    "                                  device, embedding_dim)\n",
    "    \n",
    "    distances, indices = faiss_index.search(\n",
    "        query_embedding.reshape(1, -1),\n",
    "        k\n",
    "    )\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        retrieved_chunks.append({\n",
    "            'text': chunks[idx],\n",
    "            'similarity': float(1 / (1 + distances[0][i])),\n",
    "            'metadata': chunk_sources[idx]\n",
    "        })\n",
    "    \n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 6: Generate Answer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, retrieved_chunks: List[Dict], \n",
    "                   generator_tokenizer, generator_model, device) -> Dict:\n",
    "    \"\"\"Generate an answer using the retrieved context.\"\"\"\n",
    "    sorted_chunks = sorted(retrieved_chunks, key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    context_parts = []\n",
    "    for chunk in sorted_chunks:\n",
    "        chunk_text = chunk['text']\n",
    "        for marker in ['Question:', 'Answer:', 'Reference Answer:']:\n",
    "            if marker in chunk_text:\n",
    "                chunk_text = chunk_text.split(marker)[0]\n",
    "        context_parts.append(chunk_text.strip())\n",
    "    \n",
    "    context = ' '.join(context_parts)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Based on the following context, please provide a detailed and accurate \"\n",
    "        f\"answer to the question. Consider all relevant information from the context.\\n\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    \n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", \n",
    "                               max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    outputs = generator_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,\n",
    "        min_length=30,\n",
    "        num_beams=4,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    generated_answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    source_documents = {chunk['metadata'].get('filename') for chunk in retrieved_chunks}\n",
    "    \n",
    "    return {\n",
    "        'answer': generated_answer,\n",
    "        'source_documents': list(source_documents),\n",
    "        'context_used': context,\n",
    "        'top_chunk_similarity': retrieved_chunks[0]['similarity'] if retrieved_chunks else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmengg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
