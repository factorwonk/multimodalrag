{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "\n",
    "from typing import List\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv\n",
    "from convfinqaloader import convfinqadfloader\n",
    "from transformers import pipeline\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', -1)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Flattion ConvFinQA JSON data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convfinqadfloader(\"data/convfinqatrain.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Combine relevant text fields for retrieval\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_text(row):\n",
    "    \"\"\"\n",
    "    Combine key text fields to form a context string.\n",
    "    Uses 'pre_text', 'dialogue_text', 'post_text', and 'execution_answer'.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    if pd.notnull(row.get('pre_text')):\n",
    "        texts.append(\"Pre-Text: \" + row['pre_text'])\n",
    "    if pd.notnull(row.get('dialogue_text')):\n",
    "        texts.append(\"Dialogue: \" + row['dialogue_text'])\n",
    "    if pd.notnull(row.get('post_text')):\n",
    "        texts.append(\"Post-Text: \" + row['post_text'])\n",
    "    if pd.notnull(row.get('execution_answer')):\n",
    "        texts.append(\"Execution Answer: \" + str(row['execution_answer']))\n",
    "    return \" | \".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'combined_text'\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a retrieval index using OpenAI Embeddings and FAISS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Get the embedding of a text string using OpenAI's API.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return np.array(response.data[0].embedding, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Compute embeddings for a list of texts.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        emb = get_embedding(text, model=engine)\n",
    "        embeddings.append(emb)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on 100 documents to save time (and credits!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 100 documents...\n"
     ]
    }
   ],
   "source": [
    "# Comment first line to embed all documents\n",
    "subset_df = df.iloc[:100].copy()\n",
    "doc_texts = subset_df['combined_text'].tolist()\n",
    "print(\"Computing embeddings for {} documents...\".format(len(doc_texts)))\n",
    "document_embeddings = compute_embeddings(doc_texts)\n",
    "embedding_dim = document_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings for cosine similarity (using inner product search)\n",
    "faiss.normalize_L2(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faiss index built with 100 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Build a Faiss index\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(document_embeddings)\n",
    "print(\"Faiss index built with {} vectors.\".format(index.ntotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataset(query, top_n=3, engine=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Given a query string, compute its embedding and retrieve the top_n similar documents using Faiss.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query, model=engine).reshape(1, -1)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = index.search(query_embedding, top_n)\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['score'] = distances[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Query the model using gpt-4o-mini\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context_docs, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Generate an answer by combining the query with retrieved context using OpenAI's GPT40-mini model.\n",
    "    A system prompt is added so the model acts as a friendly financial analyst bot that does not make up answers\n",
    "    and only uses the data it has access to in order to answer numerical questions.\n",
    "    \n",
    "    Parameters:\n",
    "      query: The question string.\n",
    "      context_docs: A list of context strings retrieved from the dataset.\n",
    "      max_tokens: Maximum number of tokens for the generated answer.\n",
    "    \n",
    "    Returns:\n",
    "      str: The generated answer.\n",
    "    \"\"\"\n",
    "    # Combine the retrieved context documents into a single string.\n",
    "    #context = \"\\n\".join(context_docs)\n",
    "    \n",
    "    # Create a messages list with a system prompt and the user's prompt.\n",
    "    messages = [\n",
    "         {\n",
    "             \"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a friendly financial analyst bot who is extremely knowledgeable about financial valuations, technical analysis and quantitative finance.\"\n",
    "                 \"Do not make up answers; only use the data provided to answer finance questions including financial calculation questions.\"\n",
    "             )\n",
    "         },\n",
    "         {\n",
    "             \"role\": \"user\",\n",
    "             \"content\": f\"Question: {query}\\nContext: {context_docs}\\nAnswer:\"\n",
    "         }\n",
    "    ]\n",
    "    \n",
    "    # Call the Chat Completion API with the messages.\n",
    "    response = openai.chat.completions.create(\n",
    "         model='gpt-4o-mini',\n",
    "         messages=messages,\n",
    "         max_tokens=max_tokens,\n",
    "         temperature=0.0,\n",
    "         top_p=1.0,\n",
    "         n=1,\n",
    "         stop=[\"\\n\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the generated answer.\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute the RAG pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top retrieved examples for query: what is the decrease in receivables compared to the same period a year ago?\n",
      "ID: Single_JKHY/2009/page_28.pdf-3, Turn index: 2, Score: 0.804\n",
      "Context snippet: Pre-Text: 26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business segment increased 14% ( 14 % ) from fiscal 2007 . all revenue components within the segmen...\n",
      "----------\n",
      "ID: Single_JKHY/2009/page_28.pdf-3, Turn index: 3, Score: 0.804\n",
      "Context snippet: Pre-Text: 26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business segment increased 14% ( 14 % ) from fiscal 2007 . all revenue components within the segmen...\n",
      "----------\n",
      "ID: Single_JKHY/2009/page_28.pdf-3, Turn index: 1, Score: 0.802\n",
      "Context snippet: Pre-Text: 26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business segment increased 14% ( 14 % ) from fiscal 2007 . all revenue components within the segmen...\n",
      "----------\n",
      "\n",
      "Generated Answer:\n",
      "The provided context does not contain specific data regarding receivables or any figures that would allow for the calculation of a decrease in receivables compared to the same period a year ago. To answer your question, I would need the actual receivables figures for both the current period and the same period a year ago. Please provide that data for further analysis.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the decrease in receivables compared to the same period a year ago?\"\n",
    "retrieved_results = query_dataset(query, top_n=3)\n",
    "print(\"\\nTop retrieved examples for query:\", query)\n",
    "context_docs = []\n",
    "for i, row in retrieved_results.iterrows():\n",
    "    snippet = row['combined_text'][:200] + \"...\" if len(row['combined_text']) > 200 else row['combined_text']\n",
    "    print(f\"ID: {row['id']}, Turn index: {row.get('turn_index')}, Score: {row['score']:.3f}\")\n",
    "    print(\"Context snippet:\", snippet)\n",
    "    print(\"----------\")\n",
    "    context_docs.append(snippet)\n",
    "    \n",
    "generated_answer = generate_answer(query, context_docs)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmengg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
