{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from convfinqaloader import convfinqadfloader\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', -1)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Flattion ConvFinQA JSON data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convfinqadfloader(\"data/convfinqatrain.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Combine relevant text fields for retrieval\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_text(row):\n",
    "    \"\"\"\n",
    "    Combine key text fields to form a context string.\n",
    "    Uses 'pre_text', 'dialogue_text' (if available), 'post_text', and 'execution_answer'.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    if pd.notnull(row.get('pre_text')):\n",
    "        texts.append(\"Pre-Text: \" + row['pre_text'])\n",
    "    if pd.notnull(row.get('dialogue_text')):\n",
    "        texts.append(\"Dialogue: \" + row['dialogue_text'])\n",
    "    if pd.notnull(row.get('post_text')):\n",
    "        texts.append(\"Post-Text: \" + row['post_text'])\n",
    "    if pd.notnull(row.get('execution_answer')):\n",
    "        texts.append(\"Execution Answer: \" + str(row['execution_answer']))\n",
    "    return \" | \".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'combined_text'\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a TF-IDF retrieval index\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix created with shape: (4378, 12743)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "print(\"TF-IDF matrix created with shape:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataset(query, top_n=3):\n",
    "    \"\"\"\n",
    "    Given a query string, retrieve the top_n similar examples based on cosine similarity.\n",
    "    \"\"\"\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_sim = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_indices = cosine_sim.argsort()[-top_n:][::-1]\n",
    "    results = df.iloc[top_indices].copy()\n",
    "    results['score'] = cosine_sim[top_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup a Gen-AI model using HF Transformers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4039b3794e15449a850761a80d07e748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d7495d62974566b5e5cc8c78642b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50fd4dfbc054466af59d1918041726d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3814b9f2c7784870b3070fc3f2e7ffa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f636e0790c404d69a48957582d99fd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7862269d6e6d4d42b89a9d2c2774166c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# We use a text-to-text generation model such as T5.\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context_docs, max_length=200):\n",
    "    # Combine the context documents\n",
    "    context = \"\\n\".join(context_docs)\n",
    "    # Construct a prompt that includes the query and the context.\n",
    "    prompt = f\"question: {query}\\ncontext: {context}\\nanswer:\"\n",
    "    # Generate the answer\n",
    "    result = generator(prompt, max_length=max_length, do_sample=False)\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute RAG pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what was the percentage increase in litigation reserves in 2012?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top retrieved examples for query: what was the percentage increase in litigation reserves in 2012?\n",
      "ID: Single_DVN/2015/page_117.pdf-2, Turn index: 0, Score: 0.268\n",
      "Context snippet: Pre-Text: devon energy corporation and subsidiaries notes to consolidated financial statements 2013 ( continued ) proved undeveloped reserves the following table presents the changes in devon 2019s to...\n",
      "----------\n",
      "ID: Single_DVN/2015/page_117.pdf-2, Turn index: 0, Score: 0.268\n",
      "Context snippet: Pre-Text: devon energy corporation and subsidiaries notes to consolidated financial statements 2013 ( continued ) proved undeveloped reserves the following table presents the changes in devon 2019s to...\n",
      "----------\n",
      "ID: Single_DVN/2015/page_117.pdf-2, Turn index: 2, Score: 0.265\n",
      "Context snippet: Pre-Text: devon energy corporation and subsidiaries notes to consolidated financial statements 2013 ( continued ) proved undeveloped reserves the following table presents the changes in devon 2019s to...\n",
      "----------\n",
      "\n",
      "Generated Answer:\n",
      "5%\n"
     ]
    }
   ],
   "source": [
    "retrieved_results = query_dataset(query, top_n=3)\n",
    "\n",
    "print(\"\\nTop retrieved examples for query:\", query)\n",
    "context_docs = []\n",
    "for i, row in retrieved_results.iterrows():\n",
    "    snippet = row['combined_text'][:200] + \"...\" if len(row['combined_text']) > 200 else row['combined_text']\n",
    "    print(f\"ID: {row['id']}, Turn index: {row.get('turn_index')}, Score: {row['score']:.3f}\")\n",
    "    print(\"Context snippet:\", snippet)\n",
    "    print(\"----------\")\n",
    "    context_docs.append(snippet)\n",
    "\n",
    "# Generate answer using the retrieved context\n",
    "generated_answer = generate_answer(query, context_docs)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmengg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
